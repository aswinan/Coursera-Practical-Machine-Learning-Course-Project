---
title: "Coursera Practical Machine Learning Course Project - Exercise Prediction"
author: "Aswin Andrianto"
date: "September 23, 2017"
output: html_document
---

#1) Original Study and Data Source
Source of data from the following study:

*Velloso, E.; Bulling, A.; Gellersen, H.; Ugulino, W.; Fuks, H. Qualitative Activity Recognition of Weight Lifting Exercises. Proceedings of 4th International Conference in Cooperation with SIGCHI (Augmented Human '13) . Stuttgart, Germany: ACM SIGCHI, 2013*

#2) Overview
The goal of this project is to “predict the manner in which they did the exercise.”

In the aforementioned study, six participants participated in a dumbell lifting exercise five different ways. They are as follows:
* (Class A) correct technique
* (Class B) throwing the elbows to the front
* (Class C) lifting the dumbbell only halfway
* (Class D) lowering the dumbbell only halfway
* (Class E) throwing the hips to the front

Class A corresponds to the correct execution of the exercise, while the other 4 classes correspond to common mistakes

#3) Data Preparation
##a. Load Library
```{r}
#Load libraries
library("caret")
```

##b. Load Data, Clean Data, and Check Dimension
```{r}
#Download the data
if(!file.exists("pml-training.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv", destfile = "pml-training.csv")}

if(!file.exists("pml-testing.csv")){download.file("https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv", destfile = "pml-testing.csv")}


#Read the training data and replace empty values by NA
trainingDataSet<- read.csv("pml-training.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
testingDataSet<- read.csv("pml-testing.csv", sep=",", header=TRUE, na.strings = c("NA","",'#DIV/0!'))
#remove columns containing NA
trainingDataSet <- trainingDataSet[,(colSums(is.na(trainingDataSet)) == 0)]
testingDataSet <- testingDataSet[,(colSums(is.na(testingDataSet)) == 0)]
#check training data dimension
dim(trainingDataSet)
```

```{r}
#check testing data dimension
dim(testingDataSet)
```

##c. Pre-Process Data

```{r}
numericalsIdx <- which(lapply(trainingDataSet, class) %in% "numeric")

preprocessModel <-preProcess(trainingDataSet[,numericalsIdx],method=c('knnImpute', 'center', 'scale'))
pre_trainingDataSet <- predict(preprocessModel, trainingDataSet[,numericalsIdx])
pre_trainingDataSet$classe <- trainingDataSet$classe

pre_testingDataSet <-predict(preprocessModel,testingDataSet[,numericalsIdx])
```

##d. Remove non-significant cariables (near-zero)
```{r}
nzv <- nearZeroVar(pre_trainingDataSet,saveMetrics=TRUE)
pre_trainingDataSet <- pre_trainingDataSet[,nzv$nzv==FALSE]

nzv <- nearZeroVar(pre_testingDataSet,saveMetrics=TRUE)
pre_testingDataSet <- pre_testingDataSet[,nzv$nzv==FALSE]
```

##e. Partition Validation Set

We shall set 75% observation training dataset to train the model and validate it on the last 70%.
```{r}
set.seed(151088)
idxTrain<- createDataPartition(pre_trainingDataSet$classe, p=3/4, list=FALSE)
training<- pre_trainingDataSet[idxTrain, ]
validation <- pre_trainingDataSet[-idxTrain, ]
dim(training) ; dim(validation)
```

#4) Train Model

Using Random Forest with 5-fold cross-validation
```{r}
library(randomForest)
modFitrf <- train(classe ~., method="rf", data=training, trControl=trainControl(method='cv'), number=5, allowParallel=TRUE, importance=TRUE )
modFitrf
```

##a. Plotting the importance of each variable
```{r}
varImpPlot(modFitrf$finalModel, sort = TRUE, type = 1, pch = 19, col = 1, cex = 0.6, main = "Importance of the Individual Principal Components")
```

The plot shows the importance of each component from most important to least important

##b. Cross Validation

Using testing dataset to check accuracy

```{r}
predValidRF <- predict(modFitrf, validation)
confus <- confusionMatrix(validation$classe, predValidRF)
confus$table
```
Based on the above testing, almost all of the variables fit this model.

Check accuracy
```{r}
accur <- postResample(validation$classe, predValidRF)
modAccuracy <- accur[[1]]
modAccuracy
```

Check Error Rate
```{r}
out_of_sample_error <- 1 - modAccuracy
out_of_sample_error
```
The estimated accuracy of the model is 99.5% with the error rate of 0.4%

#5) Apply prediction on the test case provided
Random Forest model will be applied to predict the 20 quiz results (testing dataset) as shown below
```{r}
pred_final <- predict(modFitrf, pre_testingDataSet)
pred_final
```